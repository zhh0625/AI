{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "pycharm": {}
      },
      "source": [
        "# Aim\n",
        "This is a small yet useful kernel for providing an introduction to **Artificial Neural Networks** for people who want to begin their journey into the field of **deep learning**. For this, I have used Keras which is a high-level Neural Networks API built on top of low level neural networks APIs like Tensorflow and Theano. As it is high-level, many things are already taken care of therefore it is easy to work with and a great tool to start with. [Here\u0027s the documentation for keras](https://keras.io/)\n",
        "\n",
        "# What is Deep learning?\n",
        "Deep Learning is a subfield of machine learning concerned with algorithms inspired by the structure and function of the brain called artificial neural networks. Deep learning is a machine learning technique that teaches computers to do what comes naturally to humans: learn by example. Deep learning is a key technology behind driverless cars, enabling them to recognize a stop sign, or to distinguish a pedestrian from a lamppost. It is the key to voice control in consumer devices like phones, tablets, TVs, and hands-free speakers. Deep learning is getting lots of attention lately and for good reason. It’s achieving results that were not possible before.\n",
        "\n",
        "\n",
        "# What are artificial neural networks?\n",
        "An artificial neuron network (ANN) is a computational model based on the structure and functions of biological neural networks. Information that flows through the network affects the structure of the ANN because a neural network changes - or learns, in a sense - based on that input and output. ANNs are considered nonlinear statistical data modeling tools where the complex relationships between inputs and outputs are modeled or patterns are found. ANN is also known as a neural network.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "collapsed": true,
        "pycharm": {}
      },
      "source": [
        "\u003cimg src\u003d\"https://cdn-images-1.medium.com/max/1000/1*ZX05x1xYgaVoa4Vn2kKS9g.png\"\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "17fa9862-de9f-4964-a688-fe9e666f0fd5",
        "_uuid": "5a1835cc73f103d85fe770ff84e0afae99ba692f",
        "pycharm": {}
      },
      "source": [
        "A single neuron is known as a perceptron. It consists of a layer of inputs(corresponds to columns of a dataframe). Each input has a weight which controls the magnitude of an input.\n",
        "The summation of the products of these input values and weights is fed to the activation function. Activation functions are really important for a Artificial Neural Network to learn and make sense of something really complicated and Non-linear complex functional mappings between the inputs and response variable. \n",
        "\n",
        "They introduce non-linear properties to our Network.Their main purpose is to convert a input signal of a node in a A-NN to an output signal. That output signal now is used as a input in the next layer in the stack.  Specifically in A-NN we do the sum of products of inputs(X) and their corresponding Weights(W) and apply a Activation function f(x) to it to get the output of that layer and feed it as an input to the next layer. [Refer to this article for more info.](https://towardsdatascience.com/activation-functions-and-its-types-which-is-better-a9a5310cc8f)\n",
        "\u003cimg src\u003d\"https://cdnpythonmachinelearning.azureedge.net/wp-content/uploads/2017/09/Single-Perceptron.png\"\u003e\n",
        "**Concept of backpropagation** - Backpropagation, short for \"backward propagation of errors,\" is an algorithm for supervised learning of artificial neural networks using gradient descent. Given an artificial neural network and an error function, the method calculates the gradient of the error function with respect to the neural network\u0027s weights. It is a generalization of the delta rule for perceptrons to multilayer feedforward neural networks.\n",
        "\u003cimg src\u003d\"https://www.researchgate.net/profile/Hassan_Al-Haj_Ibrahim/publication/235338024/figure/fig6/AS:299794191929349@1448487913220/Flow-chart-for-the-back-propagation-BP-learning-algorithm.png\"\u003e\n",
        "**Gradient Descent** - To explain Gradient Descent I’ll use the classic mountaineering example. Suppose you are at the top of a mountain, and you have to reach a lake which is at the lowest point of the mountain (a.k.a valley). A twist is that you are blindfolded and you have zero visibility to see where you are headed. So, what approach will you take to reach the lake? The best way is to check the ground near you and observe where the land tends to descend. This will give an idea in what direction you should take your first step. If you follow the descending path, it is very likely you would reach the lake. [Refer to this article for more information.](https://www.analyticsvidhya.com/blog/2017/03/introduction-to-gradient-descent-algorithm-along-its-variants/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "0de730e5-6631-4d4f-882e-4c34b4a0c37a",
        "_uuid": "e43e4e6f84cb95700e1d05f07f20fb740bcdfd31",
        "pycharm": {}
      },
      "source": [
        "About Breast Cancer Wisconsin (Diagnostic) Data Set\n",
        "Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image. n the 3-dimensional space is that described in: [K. P. Bennett and O. L. Mangasarian: \"Robust Linear Programming Discrimination of Two Linearly Inseparable Sets\", Optimization Methods and Software 1, 1992, 23-34].\n",
        "\n",
        "This database is also available through the UW CS ftp server: ftp ftp.cs.wisc.edu cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
        "\n",
        "Also can be found on UCI Machine Learning Repository: https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29\n",
        "\n",
        "Attribute Information:\n",
        "\n",
        "1) ID number 2) Diagnosis (M \u003d malignant, B \u003d benign) 3-32)\n",
        "\n",
        "Ten real-valued features are computed for each cell nucleus:\n",
        "\n",
        "a) radius (mean of distances from center to points on the perimeter) b) texture (standard deviation of gray-scale values) c) perimeter d) area e) smoothness (local variation in radius lengths) f) compactness (perimeter^2 / area - 1.0) g) concavity (severity of concave portions of the contour) h) concave points (number of concave portions of the contour) i) symmetry j) fractal dimension (\"coastline approximation\" - 1)\n",
        "\n",
        "The mean, standard error and \"worst\" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.\n",
        "\n",
        "All feature values are recoded with four significant digits.\n",
        "\n",
        "Missing attribute values: none\n",
        "\n",
        "Class distribution: 357 benign, 212 malignant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "_cell_guid": "466b1182-f052-4452-abb5-4fbe3cd92095",
        "_uuid": "6f747b567b4b89f9aaa48e503ddddabf214cb278",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": [
        "# Importing libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Importing data\n",
        "data \u003d pd.read_csv(\u0027D:/AI/实验2/1、数据/data.csv\u0027)\n",
        "del data[\u0027Unnamed: 32\u0027]                      #######删掉噪声"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "_cell_guid": "962b3671-f704-438d-bd54-931767e877cf",
        "_uuid": "f6af18d53dcaabb44c49b50c13ba3dafe7bbae10",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": [
        "X \u003d data.iloc[:, 2:].values                 #######拆分特征和标签\n",
        "y \u003d data.iloc[:, 1].values\n",
        "\n",
        "# Encoding categorical data\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "labelencoder_X_1 \u003d LabelEncoder()              ##########将字母标签转为数字形式\n",
        "y \u003d labelencoder_X_1.fit_transform(y)\n",
        "\n",
        "\n",
        "# Splitting the dataset into the Training set and Test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test \u003d train_test_split(X, y, test_size \u003d 0.1, random_state \u003d 0)\n",
        "\n",
        "#Feature Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc \u003d StandardScaler()\n",
        "X_train \u003d sc.fit_transform(X_train)       #fit_transform()的作用就是先拟合数据，然后转化它将其转化为标准形式。\n",
        "X_test \u003d sc.transform(X_test)             #已经找到了转换规则，我们把这个规则利用在训练集上，同样，我们可以直接将其运用到测试集上"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a0d8f2ca-a51c-44e5-9399-345ec7f36509",
        "_uuid": "d9f057a42b57df5d2ad73b20fc6d81b5e33628e9",
        "pycharm": {}
      },
      "source": [
        "**Now that we have prepared data, we will import Keras and its packages.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "_cell_guid": "51ab3d86-d1fd-40b6-a463-012e24a5c0a0",
        "_uuid": "c01850e3c738b8a55146cce28e5f6fe82378032e",
        "pycharm": {
          "is_executing": false
        },
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "_cell_guid": "5e19dee2-9602-42c9-bc3b-41d912e0aa1d",
        "_uuid": "f07a3637824959ff96269ff3806b994c4f64d117",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": [
        "# Initialising the ANN\n",
        "classifier \u003d Sequential()                        #建立顺序模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "_cell_guid": "b1338309-a776-44b0-b04a-a8bdbf25d10e",
        "_uuid": "cce53ac057db311d8221c027afe2d286bf0a5b51",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\program files\\python36\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation\u003d\"relu\", input_dim\u003d30, units\u003d16, kernel_initializer\u003d\"uniform\")`\n",
            "  \n",
            "c:\\program files\\python36\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Update your `Dropout` call to the Keras 2 API: `Dropout(rate\u003d0.1)`\n",
            "  after removing the cwd from sys.path.\n"
          ]
        }
      ],
      "source": [
        "# Adding the input layer and the first hidden layer\n",
        "classifier.add(Dense(output_dim\u003d16, init\u003d\u0027uniform\u0027, activation\u003d\u0027relu\u0027, input_dim\u003d30))        #output_dim表示该层神经元是16个，\u0027uniform\u0027表示用均匀分布去初始化，activation代表激活函数，input_dim代表输入层特征个数\n",
        "# Adding dropout to prevent overfitting\n",
        "classifier.add(Dropout(p\u003d0.1))                    #在隐藏层中删掉一些神经元，增加随机性，避免过拟合"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "dd954d81-d773-4e90-aaa3-34d1dcb99e68",
        "_uuid": "3d42817e7446a061dca811ea75ece45b15a4fdd9",
        "pycharm": {}
      },
      "source": [
        "input_dim - number of columns of the dataset \n",
        "\n",
        "output_dim - number of outputs to be fed to the next layer, if any\n",
        "\n",
        "activation - activation function which is ReLU in this case\n",
        "\n",
        "init - the way in which weights should be provided to an ANN\n",
        " \n",
        "The **ReLU** function is f(x)\u003dmax(0,x). Usually this is applied element-wise to the output of some other function, such as a matrix-vector product. In MLP usages, rectifier units replace all other activation functions except perhaps the readout layer. But I suppose you could mix-and-match them if you\u0027d like. One way ReLUs improve neural networks is by speeding up training. The gradient computation is very simple (either 0 or 1 depending on the sign of x). Also, the computational step of a ReLU is easy: any negative elements are set to 0.0 -- no exponentials, no multiplication or division operations. Gradients of logistic and hyperbolic tangent networks are smaller than the positive portion of the ReLU. This means that the positive portion is updated more rapidly as training progresses. However, this comes at a cost. The 0 gradient on the left-hand side is has its own problem, called \"dead neurons,\" in which a gradient update sets the incoming values to a ReLU such that the output is always zero; modified ReLU units such as ELU (or Leaky ReLU etc.) can minimize this. Source : [StackExchange](https://stats.stackexchange.com/questions/226923/why-do-we-use-relu-in-neural-networks-and-how-do-we-use-it)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "_cell_guid": "7d6c2e16-5ffb-43b1-b238-c7434faa8808",
        "_uuid": "6105fb90265fdd6082648004657d55a43b187217",
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\program files\\python36\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation\u003d\"relu\", units\u003d16, kernel_initializer\u003d\"uniform\")`\n",
            "  \n",
            "c:\\program files\\python36\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Update your `Dropout` call to the Keras 2 API: `Dropout(rate\u003d0.1)`\n",
            "  after removing the cwd from sys.path.\n"
          ]
        }
      ],
      "source": [
        "# Adding the second hidden layer\n",
        "classifier.add(Dense(output_dim\u003d16, init\u003d\u0027uniform\u0027, activation\u003d\u0027relu\u0027))\n",
        "# Adding dropout to prevent overfitting\n",
        "classifier.add(Dropout(p\u003d0.1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "_cell_guid": "628b8221-c76b-40bd-b5b9-b5aff04cbf31",
        "_uuid": "ed51c97f1ea1a10af8bee99604de076b92cb2627",
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\program files\\python36\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation\u003d\"sigmoid\", units\u003d1, kernel_initializer\u003d\"uniform\")`\n",
            "  \n"
          ]
        }
      ],
      "source": [
        "# Adding the output layer\n",
        "classifier.add(Dense(output_dim\u003d1, init\u003d\u0027uniform\u0027, activation\u003d\u0027sigmoid\u0027))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "8f80ed07-4a4a-48a4-aa84-b53fc713c61b",
        "_uuid": "71c445f1e371d37d28f4ff405b45d5497ea8bdde",
        "pycharm": {}
      },
      "source": [
        "output_dim is 1 as we want only 1 output from the final layer.\n",
        "\n",
        "Sigmoid function is used when dealing with classfication problems with 2 types of results.(Submax function is used for 3 or more classification results)\n",
        "\u003cimg src\u003d\"https://cdn-images-1.medium.com/max/1000/1*Xu7B5y9gp0iL5ooBj7LtWw.png\"\u003e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "_cell_guid": "9fc5ca11-edd6-4ddd-92b0-c5b6866abbec",
        "_uuid": "01ff691f323d264792730ae9d3af72a1e9106271",
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "# Compiling the ANN\n",
        "from keras.optimizers import Adam\n",
        "adam \u003d Adam(lr\u003d0.01)\n",
        "classifier.compile(optimizer\u003dadam, loss\u003d\u0027binary_crossentropy\u0027, metrics\u003d[\u0027accuracy\u0027])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "6375e2f2-5cf5-42e4-a199-338dbc34a07a",
        "_uuid": "d198bbf1616b2bcd99d138f24559d0eff2a07d01",
        "pycharm": {}
      },
      "source": [
        "Optimizer is chosen as adam for gradient descent.\n",
        "\n",
        "Binary_crossentropy is the loss function used. \n",
        "\n",
        "Cross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label. So predicting a probability of .012 when the actual observation label is 1 would be bad and result in a high loss value. A perfect model would have a log loss of 0. [More about this](http://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "_cell_guid": "deb09534-714c-4fd2-bac5-d5dff9c474ef",
        "_uuid": "6e2d2d3f17102bc9c4e8f093c914c58412b28ee3",
        "pycharm": {},
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 475us/step - loss: 0.6813 - acc: 0.7090\n",
            "Epoch 2/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 10us/step - loss: 0.5340 - acc: 0.9453\n",
            "Epoch 3/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 10us/step - loss: 0.2407 - acc: 0.9492\n",
            "Epoch 4/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.1175 - acc: 0.9590\n",
            "Epoch 5/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0872 - acc: 0.9766\n",
            "Epoch 6/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 10us/step - loss: 0.0760 - acc: 0.9746\n",
            "Epoch 7/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0731 - acc: 0.9746\n",
            "Epoch 8/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 14us/step - loss: 0.0659 - acc: 0.9824\n",
            "Epoch 9/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 14us/step - loss: 0.0636 - acc: 0.9844\n",
            "Epoch 10/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 14us/step - loss: 0.0576 - acc: 0.9863\n",
            "Epoch 11/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0494 - acc: 0.9883\n",
            "Epoch 12/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0549 - acc: 0.9863\n",
            "Epoch 13/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 14us/step - loss: 0.0554 - acc: 0.9883\n",
            "Epoch 14/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 14us/step - loss: 0.0469 - acc: 0.9902\n",
            "Epoch 15/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 16us/step - loss: 0.0531 - acc: 0.9883\n",
            "Epoch 16/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0470 - acc: 0.9883\n",
            "Epoch 17/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0449 - acc: 0.9863\n",
            "Epoch 18/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 10us/step - loss: 0.0435 - acc: 0.9922\n",
            "Epoch 19/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 10us/step - loss: 0.0407 - acc: 0.9902\n",
            "Epoch 20/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 10us/step - loss: 0.0444 - acc: 0.9883\n",
            "Epoch 21/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 10us/step - loss: 0.0426 - acc: 0.9883\n",
            "Epoch 22/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 10us/step - loss: 0.0367 - acc: 0.9922\n",
            "Epoch 23/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0352 - acc: 0.9883\n",
            "Epoch 24/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 10us/step - loss: 0.0364 - acc: 0.9883\n",
            "Epoch 25/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0397 - acc: 0.9902\n",
            "Epoch 26/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0340 - acc: 0.9922\n",
            "Epoch 27/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0358 - acc: 0.9902\n",
            "Epoch 28/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0308 - acc: 0.9922\n",
            "Epoch 29/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 14us/step - loss: 0.0320 - acc: 0.9922\n",
            "Epoch 30/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0271 - acc: 0.9922\n",
            "Epoch 31/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0362 - acc: 0.9902\n",
            "Epoch 32/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 10us/step - loss: 0.0299 - acc: 0.9922\n",
            "Epoch 33/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0392 - acc: 0.9883\n",
            "Epoch 34/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0284 - acc: 0.9902\n",
            "Epoch 35/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 10us/step - loss: 0.0306 - acc: 0.9883\n",
            "Epoch 36/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 14us/step - loss: 0.0248 - acc: 0.9922\n",
            "Epoch 37/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0231 - acc: 0.9922\n",
            "Epoch 38/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0262 - acc: 0.9922\n",
            "Epoch 39/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0261 - acc: 0.9922\n",
            "Epoch 40/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 10us/step - loss: 0.0177 - acc: 0.9941\n",
            "Epoch 41/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0191 - acc: 0.9922\n",
            "Epoch 42/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0211 - acc: 0.9902\n",
            "Epoch 43/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0231 - acc: 0.9922\n",
            "Epoch 44/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 10us/step - loss: 0.0186 - acc: 0.9922\n",
            "Epoch 45/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0158 - acc: 0.9961\n",
            "Epoch 46/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 14us/step - loss: 0.0116 - acc: 0.9961\n",
            "Epoch 47/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0171 - acc: 0.9922\n",
            "Epoch 48/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0097 - acc: 0.9980\n",
            "Epoch 49/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 10us/step - loss: 0.0125 - acc: 0.9961\n",
            "Epoch 50/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 14us/step - loss: 0.0274 - acc: 0.9922\n",
            "Epoch 51/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0161 - acc: 0.9961\n",
            "Epoch 52/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0245 - acc: 0.9922\n",
            "Epoch 53/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 10us/step - loss: 0.0286 - acc: 0.9922\n",
            "Epoch 54/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0234 - acc: 0.9922\n",
            "Epoch 55/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 10us/step - loss: 0.0075 - acc: 0.9980\n",
            "Epoch 56/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 10us/step - loss: 0.0205 - acc: 0.9902\n",
            "Epoch 57/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0186 - acc: 0.9961\n",
            "Epoch 58/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 10us/step - loss: 0.0088 - acc: 1.0000\n",
            "Epoch 59/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0092 - acc: 0.9980\n",
            "Epoch 60/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0224 - acc: 0.9883\n",
            "Epoch 61/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0074 - acc: 0.9980\n",
            "Epoch 62/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 10us/step - loss: 0.0231 - acc: 0.9902\n",
            "Epoch 63/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 10us/step - loss: 0.0064 - acc: 0.9961\n",
            "Epoch 64/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0180 - acc: 0.9941\n",
            "Epoch 65/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0343 - acc: 0.9941\n",
            "Epoch 66/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0240 - acc: 0.9883\n",
            "Epoch 67/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 10us/step - loss: 0.0316 - acc: 0.9941\n",
            "Epoch 68/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 10us/step - loss: 0.1419 - acc: 0.9844\n",
            "Epoch 69/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0505 - acc: 0.9863\n",
            "Epoch 70/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 10us/step - loss: 0.0436 - acc: 0.9844\n",
            "Epoch 71/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0173 - acc: 0.9961\n",
            "Epoch 72/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0327 - acc: 0.9883\n",
            "Epoch 73/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0222 - acc: 0.9902\n",
            "Epoch 74/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 16us/step - loss: 0.0322 - acc: 0.9902\n",
            "Epoch 75/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 14us/step - loss: 0.0232 - acc: 0.9941\n",
            "Epoch 76/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0242 - acc: 0.9922\n",
            "Epoch 77/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0191 - acc: 0.9902\n",
            "Epoch 78/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0227 - acc: 0.9902\n",
            "Epoch 79/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 10us/step - loss: 0.0218 - acc: 0.9922\n",
            "Epoch 80/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 14us/step - loss: 0.0229 - acc: 0.9883\n",
            "Epoch 81/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0135 - acc: 0.9961\n",
            "Epoch 82/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0229 - acc: 0.9922\n",
            "Epoch 83/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0201 - acc: 0.9941\n",
            "Epoch 84/150\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0123 - acc: 0.9961\n",
            "Epoch 85/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0175 - acc: 0.9922\n",
            "Epoch 86/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0143 - acc: 0.9941\n",
            "Epoch 87/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0236 - acc: 0.9883\n",
            "Epoch 88/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 10us/step - loss: 0.0105 - acc: 0.9961\n",
            "Epoch 89/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0115 - acc: 0.9961\n",
            "Epoch 90/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 14us/step - loss: 0.0188 - acc: 0.9961\n",
            "Epoch 91/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 10us/step - loss: 0.0102 - acc: 0.9980\n",
            "Epoch 92/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 10us/step - loss: 0.0143 - acc: 0.9941\n",
            "Epoch 93/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0117 - acc: 0.9941\n",
            "Epoch 94/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 10us/step - loss: 0.0151 - acc: 0.9941\n",
            "Epoch 95/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0161 - acc: 0.9961\n",
            "Epoch 96/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0163 - acc: 0.9941\n",
            "Epoch 97/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 10us/step - loss: 0.0226 - acc: 0.9941\n",
            "Epoch 98/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 10us/step - loss: 0.0118 - acc: 0.9961\n",
            "Epoch 99/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0082 - acc: 0.9961\n",
            "Epoch 100/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0059 - acc: 0.9980\n",
            "Epoch 101/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0098 - acc: 0.9980\n",
            "Epoch 102/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 10us/step - loss: 0.0085 - acc: 0.9961\n",
            "Epoch 103/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 10us/step - loss: 0.0088 - acc: 0.9961\n",
            "Epoch 104/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0149 - acc: 0.9941\n",
            "Epoch 105/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0048 - acc: 1.0000\n",
            "Epoch 106/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0046 - acc: 1.0000\n",
            "Epoch 107/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 10us/step - loss: 0.0043 - acc: 1.0000\n",
            "Epoch 108/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 14us/step - loss: 0.0042 - acc: 1.0000\n",
            "Epoch 109/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0153 - acc: 0.9961\n",
            "Epoch 110/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - ETA: 0s - loss: 0.0082 - acc: 1.000 - 0s 14us/step - loss: 0.0054 - acc: 1.0000\n",
            "Epoch 111/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0064 - acc: 0.9980\n",
            "Epoch 112/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 10us/step - loss: 0.0096 - acc: 0.9980\n",
            "Epoch 113/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0047 - acc: 1.0000\n",
            "Epoch 114/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0110 - acc: 0.9961\n",
            "Epoch 115/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0047 - acc: 0.9980\n",
            "Epoch 116/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 10us/step - loss: 0.0052 - acc: 0.9980\n",
            "Epoch 117/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0055 - acc: 1.0000\n",
            "Epoch 118/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 10us/step - loss: 0.0074 - acc: 0.9980\n",
            "Epoch 119/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0035 - acc: 1.0000\n",
            "Epoch 120/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 10us/step - loss: 0.0041 - acc: 1.0000\n",
            "Epoch 121/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0048 - acc: 0.9980\n",
            "Epoch 122/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - ETA: 0s - loss: 0.0019 - acc: 1.000 - 0s 10us/step - loss: 0.0023 - acc: 1.0000\n",
            "Epoch 123/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0031 - acc: 1.0000\n",
            "Epoch 124/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0045 - acc: 1.0000\n",
            "Epoch 125/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0027 - acc: 1.0000\n",
            "Epoch 126/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 10us/step - loss: 0.0039 - acc: 1.0000\n",
            "Epoch 127/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0102 - acc: 0.9980\n",
            "Epoch 128/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0026 - acc: 1.0000\n",
            "Epoch 129/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0024 - acc: 1.0000\n",
            "Epoch 130/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 10us/step - loss: 0.0057 - acc: 0.9961\n",
            "Epoch 131/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 10us/step - loss: 0.0041 - acc: 0.9980\n",
            "Epoch 132/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0057 - acc: 0.9961\n",
            "Epoch 133/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 10us/step - loss: 0.0020 - acc: 1.0000\n",
            "Epoch 134/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0127 - acc: 0.9961\n",
            "Epoch 135/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 10us/step - loss: 0.0025 - acc: 1.0000\n",
            "Epoch 136/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 10us/step - loss: 0.0033 - acc: 1.0000\n",
            "Epoch 137/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0024 - acc: 1.0000\n",
            "Epoch 138/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0011 - acc: 1.0000\n",
            "Epoch 139/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 10us/step - loss: 0.0098 - acc: 0.9980\n",
            "Epoch 140/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 16us/step - loss: 0.0014 - acc: 1.0000\n",
            "Epoch 141/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0057 - acc: 0.9980\n",
            "Epoch 142/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0021 - acc: 1.0000\n",
            "Epoch 143/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 14us/step - loss: 0.0085 - acc: 0.9961\n",
            "Epoch 144/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0091 - acc: 0.9941\n",
            "Epoch 145/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0020 - acc: 1.0000\n",
            "Epoch 146/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 14us/step - loss: 0.0025 - acc: 1.0000\n",
            "Epoch 147/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 10us/step - loss: 0.0019 - acc: 1.0000\n",
            "Epoch 148/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0020 - acc: 1.0000\n",
            "Epoch 149/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 10us/step - loss: 0.0040 - acc: 0.9980\n",
            "Epoch 150/150\n",
            "512/512 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 0s 12us/step - loss: 0.0018 - acc: 1.0000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\u003ckeras.callbacks.History at 0x1e69f348c50\u003e"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Fitting the ANN to the Training set\n",
        "classifier.fit(X_train, y_train, batch_size\u003d100, epochs\u003d150)\n",
        "# Long scroll ahead but worth\n",
        "# The batch size and number of epochs have been set using trial and error. Still looking for more efficient ways. Open to suggestions. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "91cb842e-c693-4ddc-938e-b25dd5fd8d64",
        "_uuid": "e700dd6d93bfc99f1fb10a1914c33764df95b79f",
        "pycharm": {}
      },
      "source": [
        "Batch size defines number of samples that going to be propagated through the network.\n",
        "\n",
        "An Epoch is a complete pass through all the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "_cell_guid": "cc405d00-4e8c-4882-8251-d83d4b269895",
        "_uuid": "2a668b12248656821e4797a95ed0a878a63d8ee1",
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "# Predicting the Test set results\n",
        "y_pred \u003d classifier.predict(X_test)\n",
        "y_pred \u003d (y_pred \u003e 0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "_cell_guid": "22c127bd-8608-4d8e-909b-4b90b7e24fee",
        "_uuid": "b4c8343b16d7022ccb77f8e64f46e96ac5fbe41c",
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "# Making the Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm \u003d confusion_matrix(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "_cell_guid": "2d631e23-de49-42ff-94e3-528f4131e97e",
        "_uuid": "5ed1db854bfc4bc4fb49782e7adcaaf25fe9d41e",
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Our accuracy is 98.24561403508771%\n"
          ]
        }
      ],
      "source": [
        "print(\"Our accuracy is {}%\".format(((cm[0][0] + cm[1][1])/57)*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "_uuid": "837036fec6bd2c2ee831bd3232804b001e472678",
        "pycharm": {}
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\u003d\u003d\n",
            "text/plain": [
              "\u003cFigure size 432x288 with 2 Axes\u003e"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "sns.heatmap(cm,annot\u003dTrue)\n",
        "plt.savefig(\u0027h.png\u0027)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b9ec3bc5-b073-45e5-aefc-a521c4dc9502",
        "_uuid": "68225ba8a764c54a035a068bf3852f6a084819e0",
        "collapsed": true,
        "pycharm": {}
      },
      "source": [
        "Thanks for reading this. May this help you on your \"deep\" journey into machine learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}