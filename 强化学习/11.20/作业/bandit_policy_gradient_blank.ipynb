{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "#List out our bandits. Currently bandit 4 (index#3) is set to most often provide a positive reward.\n",
        "bandits \u003d [0.2,0,-0.2,-5]\n",
        "num_bandits \u003d len(bandits)\n",
        "def pullBandit(bandit):\n",
        "    #Get a random number.\n",
        "    result \u003d np.random.randn(1)\n",
        "    if result \u003e bandit:\n",
        "        #return a positive reward.\n",
        "        return 1\n",
        "    else:\n",
        "        #return a negative reward.\n",
        "        return -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": "tf.reset_default_graph()\n\n#These two lines established the feed-forward part of the network. This does the actual choosing.\nweights \u003d tf.Variable(tf.ones([num_bandits]))\nchosen_action \u003d tf.argmax(weights,0)\n\n#The next six lines establish the training proceedure. We feed the reward and chosen action into the network\n#to compute the loss, and use it to update the network.\nreward_holder \u003d tf.placeholder(shape\u003d[1],dtype\u003dtf.float32)\naction_holder \u003d tf.placeholder(shape\u003d[1],dtype\u003dtf.int32)\nresponsible_weight \u003d tf.slice(weights,action_holder,[1])\nloss\u003d-(tf.log(responsible_weight)*reward_holder)#参照ppt，实现policy gradient的loss function\noptimizer \u003d tf.train.GradientDescentOptimizer(learning_rate\u003d0.001)\nupdate \u003d optimizer.minimize(loss)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": "total_episodes \u003d 1000 #Set total number of episodes to train agent on.\ntotal_reward \u003d np.zeros(num_bandits) #Set scoreboard for bandits to 0.\ne \u003d 0.1 #Set the chance of taking a random action.\n\ninit \u003d tf.initialize_all_variables()\n\n# Launch the tensorflow graph\nwith tf.Session() as sess:\n    sess.run(init)\n    i \u003d 0\n    while i \u003c total_episodes:\n        \n        #Choose either a random action or one from our network.\n        if np.random.rand(1) \u003c e:#利用e实现e-greedy\n            action\u003dnp.random.randint(num_bandits)\n        else:\n            action\u003dsess.run(chosen_action)#利用e实现e-greedy\n        \n        reward \u003d pullBandit(bandits[action]) #Get our reward from picking one of the bandits.\n        \n        #Update the network.\n        _,resp,ww \u003d sess.run([update,responsible_weight,weights], feed_dict\u003d{reward_holder:[reward],action_holder:[action]})\n        \n        #Update our running tally of scores.\n        total_reward[action] +\u003d reward\n        if i % 50 \u003d\u003d 0:\n            print (\"Running reward for the \" + str(num_bandits) + \" bandits: \" + str(total_reward))\n        i+\u003d1\nprint (\"The agent thinks bandit \" + str(np.argmax(ww)+1) + \" is the most promising....\")\nif np.argmax(ww) \u003d\u003d np.argmax(-np.array(bandits)):\n    print (\"...and it was right!\")\nelse:\n    print (\"...and it was wrong!\")"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}