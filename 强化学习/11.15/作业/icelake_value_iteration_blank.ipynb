{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Solving FrozenLake8x8 environment using Value-Itertion.\n",
        "Author : Moustafa Alzantot (malzantot@ucla.edu)\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import gym\n",
        "from gym import wrappers\n",
        "\n",
        "\n",
        "def run_episode(env, policy, gamma \u003d 1.0, render \u003d False):\n",
        "    \"\"\" Evaluates policy by using it to run an episode and finding its\n",
        "    total reward.\n",
        "    args:\n",
        "    env: gym environment.\n",
        "    policy: the policy to be used.\n",
        "    gamma: discount factor.\n",
        "    render: boolean to turn rendering on/off.\n",
        "    returns:\n",
        "    total reward: real value of the total reward recieved by agent under policy.\n",
        "    \"\"\"\n",
        "    obs \u003d env.reset()\n",
        "    total_reward \u003d 0\n",
        "    step_idx \u003d 0\n",
        "    while True:\n",
        "        if render:\n",
        "            env.render()\n",
        "        obs, reward, done , _ \u003d env.step(int(policy[obs]))\n",
        "        total_reward +\u003d (gamma ** step_idx * reward)\n",
        "        step_idx +\u003d 1\n",
        "        if done:\n",
        "            break\n",
        "    return total_reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": [
        "def evaluate_policy(env, policy, gamma \u003d 1.0,  n \u003d 100):\n",
        "    \"\"\" Evaluates a policy by running it n times.\n",
        "    returns:\n",
        "    average total reward\n",
        "    \"\"\"\n",
        "    scores \u003d [\n",
        "            run_episode(env, policy, gamma \u003d gamma, render \u003d False)\n",
        "            for _ in range(n)]\n",
        "    return np.mean(scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": "def extract_policy(v, gamma \u003d 1.0):\n    \"\"\" Extract the policy given a value-function \n    在一个收敛的、能够对状态进行准确评估的状态值函数的基础上，\n    推导出策略函数，即在每一个状态下应该采取什么动作最优的\"\"\"\n    policy \u003d np.zeros(env.nS)\n    # policy代表处于状态t时应该采取的最佳动作是上/下/左/右,policy长度\n    for s in range(env.nS):\n        # 将价值迭代的过程再走一遍，但是不再更新value function，\n        # 而是选出每个状态下对应最大价值的动作\n        q_sa \u003d np.zeros(env.action_space.n)\n        for a in range(env.action_space.n):\n            for next_sr in env.P[s][a]:\n                # next_sr is a tuple of (probability, next state, reward, done)\n                p, s_, r, _ \u003d next_sr\n                q_sa[a] +\u003d(p*(r+gamma*v[s_]))          #根据ppt RL2-1 4页的Bellman方程的第二个方法，计算q_sa\n        policy[s] \u003dnp.argmax(q_sa)         #贪婪式的更新policy ppt RL2-1 21页\n    return policy"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": "def value_iteration(env, gamma \u003d 1.0):\n    \"\"\" Value-iteration algorithm \n    value值迭代函数，目的是准确评估每一个状态的好坏\"\"\"\n    v \u003d np.zeros(env.nS)  #  初始化价值函数，向量维度等于游戏状态的个数\n    max_iterations \u003d 100000\n    eps \u003d 1e-20\n    for i in range(max_iterations):\n        prev_v \u003d np.copy(v)  #prev_v的长度为8\n        for s in range(env.nS):#state\u003d0，1，2，...,63\n            # 在某个状态下，采取4种动作后，分别的reward\n            for a in range(env.nA):#action\u003d0,1,2,3\n                # 采取不同action转移到不同的状态，也对应不同的reward\n                q_sa \u003d [sum([p*(r+prev_v[s_]) for p, s_, r, _ in env.P[s][a]]) for a in range(env.nA)] #根据ppt RL2-1 28页的方法，计算v\n                v[s] \u003d max(q_sa)\n        if (np.sum(np.fabs(prev_v - v)) \u003c\u003d eps):\n            print (\u0027Value-iteration converged at iteration# %d.\u0027 %(i+1))\n            break\n    return v"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "Value-iteration converged at iteration# 52.\nPolicy average score \u003d  0.0\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "if __name__ \u003d\u003d \u0027__main__\u0027:\n    env_name  \u003d \u0027FrozenLake8x8-v0\u0027\n    gamma \u003d 1.0\n    env \u003d gym.make(env_name)\n    optimal_v \u003dvalue_iteration(env, gamma \u003d 1.0)  #使用value迭代法得到最优的v值\n    policy \u003dextract_policy(optimal_v, gamma \u003d 1.0)   #根据最好的v值，获取最优policy\n    policy_score \u003d evaluate_policy(env, policy, gamma, n\u003d100)\n    print(\u0027Policy average score \u003d \u0027, policy_score)"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}