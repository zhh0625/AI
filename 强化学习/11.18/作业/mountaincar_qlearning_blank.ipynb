{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "----- using Q Learning -----\nIteration #1 -- Total reward \u003d -200.\n",
            "Iteration #101 -- Total reward \u003d -200.\n",
            "Iteration #201 -- Total reward \u003d -200.\nIteration #301 -- Total reward \u003d -200.",
            "\nIteration #401 -- Total reward \u003d -200.\nIteration #501 -- Total reward \u003d -200.",
            "\n",
            "Iteration #601 -- Total reward \u003d -200.\n",
            "Iteration #701 -- Total reward \u003d -200.\n",
            "Iteration #801 -- Total reward \u003d -200.\nIteration #901 -- Total reward \u003d -200.\n",
            "Iteration #1001 -- Total reward \u003d -200.\nIteration #1101 -- Total reward \u003d -200.\nIteration #1201 -- Total reward \u003d -200.\n",
            "Iteration #1301 -- Total reward \u003d -200.\nIteration #1401 -- Total reward \u003d -200.",
            "\nIteration #1501 -- Total reward \u003d -200.\nIteration #1601 -- Total reward \u003d -200.\nIteration #1701 -- Total reward \u003d -200.\n",
            "Iteration #1801 -- Total reward \u003d -200.\nIteration #1901 -- Total reward \u003d -200.",
            "\nIteration #2001 -- Total reward \u003d -200.\n",
            "Iteration #2101 -- Total reward \u003d -200.\nIteration #2201 -- Total reward \u003d -200.",
            "\nIteration #2301 -- Total reward \u003d -200.\n",
            "Iteration #2401 -- Total reward \u003d -200.\nIteration #2501 -- Total reward \u003d -200.\n",
            "Iteration #2601 -- Total reward \u003d -200.\n",
            "Iteration #2701 -- Total reward \u003d -200.\n",
            "Iteration #2801 -- Total reward \u003d -200.\n",
            "Iteration #2901 -- Total reward \u003d -200.\n",
            "Iteration #3001 -- Total reward \u003d -200.\n",
            "Iteration #3101 -- Total reward \u003d -200.\n",
            "Iteration #3201 -- Total reward \u003d -200.\n",
            "Iteration #3301 -- Total reward \u003d -200.\n",
            "Iteration #3401 -- Total reward \u003d -200.\nIteration #3501 -- Total reward \u003d -200.\n",
            "Iteration #3601 -- Total reward \u003d -200.\n",
            "Iteration #3701 -- Total reward \u003d -200.\n",
            "Iteration #3801 -- Total reward \u003d -200.\n",
            "Iteration #3901 -- Total reward \u003d -200.\n",
            "Iteration #4001 -- Total reward \u003d -200.\n",
            "Iteration #4101 -- Total reward \u003d -200.\n",
            "Iteration #4201 -- Total reward \u003d -200.\n",
            "Iteration #4301 -- Total reward \u003d -200.\n",
            "Iteration #4401 -- Total reward \u003d -200.\n",
            "Iteration #4501 -- Total reward \u003d -200.\n",
            "Iteration #4601 -- Total reward \u003d -200.\n",
            "Iteration #4701 -- Total reward \u003d -200.\n",
            "Iteration #4801 -- Total reward \u003d -200.\n",
            "Iteration #4901 -- Total reward \u003d -200.\n",
            "Iteration #5001 -- Total reward \u003d -200.\n",
            "Iteration #5101 -- Total reward \u003d -200.\n",
            "Iteration #5201 -- Total reward \u003d -200.\n",
            "Iteration #5301 -- Total reward \u003d -200.\n",
            "Iteration #5401 -- Total reward \u003d -200.\n",
            "Iteration #5501 -- Total reward \u003d -200.\n",
            "Iteration #5601 -- Total reward \u003d -200.\n",
            "Iteration #5701 -- Total reward \u003d -200.\nIteration #5801 -- Total reward \u003d -200.\n",
            "Iteration #5901 -- Total reward \u003d -200.\nIteration #6001 -- Total reward \u003d -200.",
            "\n",
            "Iteration #6101 -- Total reward \u003d -200.\nIteration #6201 -- Total reward \u003d -200.\nIteration #6301 -- Total reward \u003d -200.",
            "\nIteration #6401 -- Total reward \u003d -200.",
            "\nIteration #6501 -- Total reward \u003d -200.\n",
            "Iteration #6601 -- Total reward \u003d -200.\n",
            "Iteration #6701 -- Total reward \u003d -200.\nIteration #6801 -- Total reward \u003d -200.",
            "\nIteration #6901 -- Total reward \u003d -200.",
            "\nIteration #7001 -- Total reward \u003d -200.\nIteration #7101 -- Total reward \u003d -200.\n",
            "Iteration #7201 -- Total reward \u003d -200.\n",
            "Iteration #7301 -- Total reward \u003d -200.\nIteration #7401 -- Total reward \u003d -200.\n",
            "Iteration #7501 -- Total reward \u003d -200.\nIteration #7601 -- Total reward \u003d -200.\n",
            "Iteration #7701 -- Total reward \u003d -200.\nIteration #7801 -- Total reward \u003d -200.\nIteration #7901 -- Total reward \u003d -200.\n",
            "Iteration #8001 -- Total reward \u003d -198.\nIteration #8101 -- Total reward \u003d -200.\n",
            "Iteration #8201 -- Total reward \u003d -200.\nIteration #8301 -- Total reward \u003d -200.",
            "\nIteration #8401 -- Total reward \u003d -200.\nIteration #8501 -- Total reward \u003d -200.\n",
            "Iteration #8601 -- Total reward \u003d -200.\n",
            "Iteration #8701 -- Total reward \u003d -200.\nIteration #8801 -- Total reward \u003d -200.\nIteration #8901 -- Total reward \u003d -200.",
            "\n",
            "Iteration #9001 -- Total reward \u003d -200.\n",
            "Iteration #9101 -- Total reward \u003d -200.\n",
            "Iteration #9201 -- Total reward \u003d -200.\n",
            "Iteration #9301 -- Total reward \u003d -200.\n",
            "Iteration #9401 -- Total reward \u003d -200.\n",
            "Iteration #9501 -- Total reward \u003d -200.\n",
            "Iteration #9601 -- Total reward \u003d -200.\n",
            "Iteration #9701 -- Total reward \u003d -200.\nIteration #9801 -- Total reward \u003d -200.\nIteration #9901 -- Total reward \u003d -200.\n",
            "Average score of solution \u003d  -129.96\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "\"\"\"\nQ-Learning example using OpenAI gym MountainCar enviornment\nAuthor: Moustafa Alzantot (malzantot@ucla.edu)\n\"\"\"\n#请补全???的部分\n#附加题：用policy iteration或value iteration解决这个例子\n\nimport numpy as np\n\nimport gym\nfrom gym import wrappers\n\nn_states \u003d 40\niter_max \u003d 10000\n\ninitial_lr \u003d 1.0 # Learning rate\nmin_lr \u003d 0.003\ngamma \u003d 1.0\nt_max \u003d 10000\neps \u003d 0.02\n\ndef run_episode(env, policy\u003dNone, render\u003dFalse):\n    obs \u003d env.reset()\n    total_reward \u003d 0\n    step_idx \u003d 0\n    for _ in range(t_max):\n        if render:\n            env.render()\n        if policy is None:\n            action \u003d env.action_space.sample()\n        else:\n            a,b\u003dobs_to_state(env,obs)          #得到离散化的状态\n            action\u003dpolicy[a][b]          #按照policy决定下次action\n        obs, reward, done, _ \u003d env.step(action)\n        total_reward +\u003d gamma ** step_idx * reward\n        step_idx +\u003d 1\n        if done:\n            break\n    return total_reward\n\ndef obs_to_state(env, obs):             #该函数的作用的是将连续的状态转换成有限的离散状态\n    \"\"\" Maps an observation to state \"\"\"\n    env_low \u003d env.observation_space.low\n    env_high \u003d env.observation_space.high\n    env_dx \u003d (env_high - env_low) / n_states\n    a \u003d int((obs[0] - env_low[0])/env_dx[0])\n    b \u003d int((obs[1] - env_low[1])/env_dx[1])\n    return a, b\n\nif __name__ \u003d\u003d \u0027__main__\u0027:\n    env_name \u003d \u0027MountainCar-v0\u0027\n    env \u003d gym.make(env_name)\n    env.seed(0)\n    np.random.seed(0)\n    print (\u0027----- using Q Learning -----\u0027)\n    q_table\u003dnp.zeros((n_states,n_states,3))  #初始化q表\n    for i in range(iter_max):\n        obs \u003d env.reset()\n        total_reward \u003d 0\n        ## eta: learning rate is decreased at each step\n        eta \u003d max(min_lr, initial_lr * (0.85 ** (i//100)))\n        for j in range(t_max):\n            a, b \u003d obs_to_state(env, obs)\n            if np.random.uniform(0, 1) \u003c eps:\n                action \u003d np.random.choice(env.action_space.n)\n            else:\n                logits \u003d q_table[a][b]\n                logits_exp \u003d np.exp(logits)\n                probs \u003d logits_exp / np.sum(logits_exp)\n                action\u003dnp.random.choice(env.action_space.n,p\u003dprobs)    #按照概率probs选择下一个action\n            obs, reward, done, _ \u003d env.step(action)\n            total_reward +\u003d (gamma ** j) * reward\n            # update q table\n            a_, b_ \u003d obs_to_state(env, obs)\n            q_table[a][b][action]\u003dq_table[a][b][action]+eta*(reward+gamma*np.max(q_table[a_][b_])-q_table[a][b][action])    #更新q表\n            if done:\n                break\n        if i % 100 \u003d\u003d 0:\n            print(\u0027Iteration #%d -- Total reward \u003d %d.\u0027 %(i+1, total_reward))\n    solution_policy \u003d np.argmax(q_table, axis\u003d2)\n    solution_policy_scores \u003d [run_episode(env, solution_policy, False) for _ in range(100)]\n    print(\"Average score of solution \u003d \", np.mean(solution_policy_scores))\n    # Animate it\n    run_episode(env, solution_policy, True)"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}